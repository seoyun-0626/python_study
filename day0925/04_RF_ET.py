import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.model_selection import train_test_split, GridSearchCV

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° 
data = fetch_california_housing()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

print(X.head())
X.info()
print(y.head())

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)

# ëª¨ë¸ ì •ì˜ (ë‘ê°œì£ )
rf_model = RandomForestRegressor(random_state=42)
et_model = ExtraTreesRegressor(random_state=42)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •
param_grid = {}
#     'n_estimators': [100, 300],   # íŠ¸ë¦¬ ê°œìˆ˜ ëŠ˜ë¦¬ê¸°`

# `
#     'max_depth': [None, 10, 20],        # íŠ¸ë¦¬ ê¹Šì´ ë‹¤ì–‘í•˜ê²Œ
#     'min_samples_split': [2, 5],        # ë¶„í•  ê¸°ì¤€ ë” ë‹¤ì–‘í•˜ê²Œ
#     'min_samples_leaf': [1, 2]           # ìµœì†Œ ì ê°œìˆ˜ ì¶”ê°€
# }

# ê·¸ë¦¬ë“œ ì„œì¹˜ ì„¤ì •
rf_grid = GridSearchCV(rf_model, param_grid, n_jobs=-1, verbose=2)
et_grid = GridSearchCV(et_model, param_grid, n_jobs=-1, verbose=2)
# ëª¨ë¸ í•™ìŠµ
print('ëœë¤í¬ë ˆìŠ¤íŠ¸ ê·¸ë¦¬ë“œ ì„œì¹˜')
rf_grid.fit(X_train, y_train)


print('ì—‘ìŠ¤íŠ¸ë¼íŠ¸ë¦¬ ê·¸ë¦¬ë“œ ì„œì¹˜')
et_grid.fit(X_train, y_train)
print()

# ìµœì ì˜ íŒŒë¼ë¯¸í„° ì¶œë ¥
print('ëœë¤í¬ë ˆìŠ¤íŠ¸ best: ', rf_grid.best_params_)
print('ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬ best:' , et_grid.best_params_)

# ìµœì  ëª¨ë¸ 
rf_best = rf_grid.best_estimator_
et_best = et_grid.best_estimator_

# ì„±ëŠ¥ í‰ê°€  
print('ëœë¤í¬ë ˆìŠ¤íŠ¸ ìŠ¤ì½”ì–´:',rf_best.score(X_test, y_test))
print('ì—‘ìŠ¤íŠ¸ë¼íŠ¸ë¦¬ ìŠ¤ì½”ì–´',et_best.score(X_test, y_test))

# ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… 
from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor(random_state=42)
gbr.fit(X_train, y_train)

print('ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… : ',gbr.score(X_test, y_test))

# XGBoost

from xgboost import XGBRegressor

xgb = XGBRegressor(
    n_estimators=1000,  # íŠ¸ë¦¬ ëª‡ ê°œ ë§Œë“¤ì§€
    learning_rate=0.08, # ê° íŠ¸ë¦¬ì˜ ê¸°ì—¬ë„ë¥¼ ì–¼ë§ˆë‚˜ ì¤„ ê±´ì§€ (í•™ìŠµë¥ )
    max_depth=6,
    subsample=0.8,  # ì „ì²´ ë°ì´í„° ì¤‘ 80%ë§Œ ëœë¤í•˜ê²Œ ì‚¬ìš©
    colsample_bytree=0.8, # íŠ¹ì„±(ì—´) ì¤‘ 80%ë§Œ ëœë¤ ì„ íƒ
    random_state=42,
    n_jobs=-1,
    verbosity=2)
'''
ğŸ“Œ verbosity ê°’
0 â†’ silent (ì¶œë ¥ ì—†ìŒ)
1 â†’ warningë§Œ ì¶œë ¥
2 â†’ info (í•™ìŠµ ì§„í–‰ìƒí™© ë¡œê·¸ ì¶œë ¥) âœ…
3 â†’ debug (ì•„ì£¼ ìƒì„¸í•œ ë¡œê·¸)
'''

xgb.fit(X_train, y_train)
print('XGBoost :', xgb.score(X_test, y_test))


