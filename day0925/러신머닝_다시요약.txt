â¤ KNN (K-Nearest Neighbors)
- ë¶„ë¥˜: ê°€ê¹Œìš´ Kê°œ ì´ì›ƒ ë‹¤ìˆ˜ê²°ë¡œ í´ë˜ìŠ¤ ê²°ì •
- íšŒê·€: ê°€ê¹Œìš´ Kê°œ ì´ì›ƒ íƒ€ê¹ƒ í‰ê·  ì˜ˆì¸¡

â¤ ì„ í˜• íšŒê·€ (Linear Regression)
- ì…ë ¥ íŠ¹ì„±ë“¤ì˜ ì„ í˜• ì¡°í•©ìœ¼ë¡œ ì˜ˆì¸¡
- ì†ì‹¤ í•¨ìˆ˜: MSE (í‰ê· ì œê³±ì˜¤ì°¨)

â¤ ë¦¿ì§€ íšŒê·€ (Ridge Regression)
- ì„ í˜• íšŒê·€ + L2 ê·œì œ
- ì†ì‹¤ í•¨ìˆ˜: MSE + Î» Î£ wÂ²
- ê³¼ì í•© ë°©ì§€ (ëª¨ë“  ê³„ìˆ˜ë¥¼ ì¡°ê¸ˆì”© ì¤„ì„)

â¤ ë¼ì˜ íšŒê·€ (Lasso Regression)
- ì„ í˜• íšŒê·€ + L1 ê·œì œ
- ì†ì‹¤ í•¨ìˆ˜: MSE + Î» Î£ |w|
- ì¼ë¶€ ê³„ìˆ˜ = 0 â†’ ë³€ìˆ˜ ì„ íƒ íš¨ê³¼

â¤ ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression)
- ë¶„ë¥˜ìš© ëª¨ë¸ (0/1, ë‹¤ì¤‘ í´ë˜ìŠ¤)
- z = wÂ·x + b â†’ sigmoid(z)
- ì†ì‹¤ í•¨ìˆ˜: Log Loss (êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤)
- ë‹¤ì¤‘ í´ë˜ìŠ¤: ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€

â¤ SGD (Stochastic Gradient Descent)
- í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•
- ëŒ€ìš©ëŸ‰ ë°ì´í„° í•™ìŠµì— ì í•©
- ë¶„ë¥˜/íšŒê·€ ëª¨ë‘ ê°€ëŠ¥ (loss ì˜µì…˜ì— ë”°ë¼)

â¤ ê²°ì • íŠ¸ë¦¬ (Decision Tree)
- if-else êµ¬ì¡°ë¡œ ë¶„ë¥˜/íšŒê·€
- ì§ê´€ì , í•´ì„ ì‰¬ì›€
- ê³¼ì í•© ì‰¬ì›€ â†’ max_depth, min_samples_split ì œí•œ í•„ìš”

â¤ ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Random Forest)
- ì—¬ëŸ¬ ê²°ì •íŠ¸ë¦¬ë¥¼ ì•™ìƒë¸”
- ë¬´ì‘ìœ„ ìƒ˜í”Œ + ë¬´ì‘ìœ„ íŠ¹ì„±
- ê³¼ì í•© ì™„í™”, ì¼ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ â†‘

â¤ ê·¸ë ˆì´ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… (Gradient Boosting)
- ì´ì „ íŠ¸ë¦¬ì˜ ì˜¤ì°¨ë¥¼ ë³´ì™„í•˜ë©° ìˆœì°¨ í•™ìŠµ
- XGBoost, LightGBM, CatBoost ë“±
- ì ì€ ë°ì´í„°ì—ì„œë„ ê°•ë ¥

---------------------------------------
â¤ ì†ì‹¤ í•¨ìˆ˜ (Loss Function)
- ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ ìˆ˜ì¹˜í™”
- íšŒê·€: MSE, MAE
- ë¶„ë¥˜: Log Loss (êµì°¨ ì—”íŠ¸ë¡œí”¼)

â¤ SSE & MSE
- SSE: ì˜¤ì°¨ ì œê³±ì˜ ì´í•©
- MSE: SSE / N â†’ í‰ê·  ì œê³± ì˜¤ì°¨

â¤ ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent)
- ì†ì‹¤ í•¨ìˆ˜ ìµœì†Œí™” ì•Œê³ ë¦¬ì¦˜
- ê¸°ìš¸ê¸°(ë¯¸ë¶„) ë°©í–¥ ë°˜ëŒ€ë¡œ ì¡°ê¸ˆì”© ì´ë™
- ì¢…ë¥˜: ë°°ì¹˜ GD, í™•ë¥ ì  GD, ë¯¸ë‹ˆë°°ì¹˜ GD

â¤ ê·œì œ (Regularization)
- ê³¼ì í•© ë°©ì§€ìš© íŒ¨ë„í‹°
- L1 (ë¼ì˜): Î£ |w| â†’ ë³€ìˆ˜ ì„ íƒ íš¨ê³¼
- L2 (ë¦¿ì§€): Î£ wÂ² â†’ ì•ˆì •ì , ê³„ìˆ˜ ì¶•ì†Œ

â¤ ì‹œê·¸ëª¨ì´ë“œ (Sigmoid)
- í™•ë¥  ì¶œë ¥ (0~1)
- Ïƒ(x) = 1 / (1 + e^-x)

â¤ ì†Œí”„íŠ¸ë§¥ìŠ¤ (Softmax)
- ë‹¤ì¤‘ í´ë˜ìŠ¤ í™•ë¥  ë¶„í¬ ì¶œë ¥
- ê° í´ë˜ìŠ¤ í™•ë¥ ì˜ í•© = 1

â¤ íŠ¹ì„± ê³µí•™ (Feature Engineering)
- ë°ì´í„°ë¥¼ ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ì¢‹ê²Œ ê°€ê³µ
- íŠ¹ì„± ì„ íƒ, ë³€í™˜(ë¡œê·¸, í‘œì¤€í™”), ìƒì„±(ë‹¤í•­ íŠ¹ì„±), ì¸ì½”ë”© ë“±

â¤ êµì°¨ ê²€ì¦ (Cross Validation, CV)
- ë°ì´í„°ë¥¼ ì—¬ëŸ¬ í´ë“œë¡œ ë‚˜ëˆ  ë²ˆê°ˆì•„ ê²€ì¦
- ë°ì´í„° ë¶„í• ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€ë™ â†“

â¤ ê·¸ë¦¬ë“œ ì„œì¹˜ (Grid Search)
- ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© íƒìƒ‰
- GridSearchCV = êµì°¨ ê²€ì¦ í¬í•¨

â¤ ëœë¤ ì„œì¹˜ (Random Search)
- ì¼ë¶€ ì¡°í•©ë§Œ ëœë¤ìœ¼ë¡œ íƒìƒ‰
- RandomizedSearchCV = êµì°¨ ê²€ì¦ í¬í•¨




ğŸ‘‰ ì‰½ê²Œ ë§í•˜ë©´:
ì²« ë²ˆì§¸ íŠ¸ë¦¬ê°€ í‹€ë¦° ë¶€ë¶„ì„ ë‘ ë²ˆì§¸ê°€ ê³ ì³ì£¼ê³ ,
ì„¸ ë²ˆì§¸ê°€ ë˜ ë¶€ì¡±í•œ ë¶€ë¶„ì„ ë©”ê¿”ì£¼ëŠ” ë¦´ë ˆì´ í•™ìŠµ.



# =============================
# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
# =============================
import numpy as np              # ìˆ˜ì¹˜ ê³„ì‚° (ë°°ì—´, ìˆ˜í•™ í•¨ìˆ˜)
import pandas as pd             # ë°ì´í„° í”„ë ˆì„ (CSV, Excel ì½ê¸°/ì“°ê¸°)

# =============================
# ë°ì´í„° ë¶„í•  / êµì°¨ ê²€ì¦
# =============================
from sklearn.model_selection import train_test_split    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„í• 
from sklearn.model_selection import cross_val_score     # êµì°¨ ê²€ì¦ ì ìˆ˜ ê³„ì‚°
from sklearn.model_selection import GridSearchCV        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ (ì™„ì „íƒìƒ‰)
from sklearn.model_selection import RandomizedSearchCV  # í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ (ëœë¤íƒìƒ‰)

# =============================
# ìµœê·¼ì ‘ ì´ì›ƒ (KNN)
# =============================
from sklearn.neighbors import KNeighborsClassifier      # KNN ë¶„ë¥˜
from sklearn.neighbors import KNeighborsRegressor       # KNN íšŒê·€

# =============================
# ì„ í˜• ëª¨ë¸ ê³„ì—´
# =============================
from sklearn.linear_model import LinearRegression       # ì„ í˜• íšŒê·€ (ê¸°ë³¸ íšŒê·€)
from sklearn.linear_model import Ridge                  # ë¦¿ì§€ íšŒê·€ (L2 ê·œì œ)
from sklearn.linear_model import Lasso                  # ë¼ì˜ íšŒê·€ (L1 ê·œì œ)
from sklearn.linear_model import LogisticRegression     # ë¡œì§€ìŠ¤í‹± íšŒê·€ (ë¶„ë¥˜, ì‹œê·¸ëª¨ì´ë“œ/ì†Œí”„íŠ¸ë§¥ìŠ¤)
from sklearn.linear_model import SGDClassifier          # SGD (ë¶„ë¥˜)
from sklearn.linear_model import SGDRegressor           # SGD (íšŒê·€)

# =============================
# ê²°ì • íŠ¸ë¦¬ ê³„ì—´
# =============================
from sklearn.tree import DecisionTreeClassifier         # ê²°ì •íŠ¸ë¦¬ ë¶„ë¥˜
from sklearn.tree import DecisionTreeRegressor          # ê²°ì •íŠ¸ë¦¬ íšŒê·€

# =============================
# ì•™ìƒë¸” (ì—¬ëŸ¬ íŠ¸ë¦¬ ì¡°í•©)
# =============================
from sklearn.ensemble import RandomForestClassifier     # ëœë¤í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜
from sklearn.ensemble import RandomForestRegressor      # ëœë¤í¬ë ˆìŠ¤íŠ¸ íšŒê·€
from sklearn.ensemble import GradientBoostingClassifier # ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¶„ë¥˜
from sklearn.ensemble import GradientBoostingRegressor  # ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… íšŒê·€

# =============================
# ì „ì²˜ë¦¬ / íŠ¹ì„± ê³µí•™
# =============================
from sklearn.preprocessing import StandardScaler        # ë°ì´í„° í‘œì¤€í™” (í‰ê· =0, ë¶„ì‚°=1)
from sklearn.preprocessing import PolynomialFeatures    # ë‹¤í•­ì‹ íŠ¹ì„± ìƒì„± (x, xÂ², xÂ³...)
from sklearn.preprocessing import OneHotEncoder         # ë²”ì£¼í˜• ë³€ìˆ˜ ì›-í•« ì¸ì½”ë”©



*****ìš”ì•½*********************************

KNN â†’ neighbors
ì„ í˜•/ë¦¿ì§€/ë¼ì˜/ë¡œì§€ìŠ¤í‹±/SGD â†’ linear_model
ê²°ì •íŠ¸ë¦¬ â†’ tree
ëœë¤í¬ë ˆìŠ¤íŠ¸/ë¶€ìŠ¤íŒ… â†’ ensemble
ë°ì´í„° ë¶„í• Â·íƒìƒ‰ â†’ model_selection
ì „ì²˜ë¦¬Â·íŠ¹ì„±ê³µí•™ â†’ preprocessing


