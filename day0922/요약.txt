❤ KNN (K-Nearest Neighbors)

분류(KNN Classifier)
→ 최근접 이웃 K개를 조사해서 가장 많은 클래스로 분류

회귀(KNN Regressor)
→ 최근접 이웃 K개의 타깃 값 평균을 예측

👉 쉽게 말하면: 
"내가 모르는 애 하나 있는데, 주변에 친한 친구 K명을 불러다가
다수결로 정답을 정하는 게 분류,
평균을 내주는 게 회귀"

-----------------------------------------------------


❤ 선형 회귀 (Linear Regression)

입력 특성들의 선형 조합으로 예측

손실 함수 = MSE(Mean Squared Error)
→ 예측값과 실제값 차이 제곱의 평균이 작아야 함

👉 쉽게 말하면: 
"y = ax + b" 같은 직선을 그려서,
데이터 점들과 그 직선 사이 거리가 최대한 작게 만드는 것.
그 거리(오차)의 제곱 평균이 MSE

--------------------------------


❤ 릿지 회귀 (Ridge Regression)

선형 회귀 + 규제(Regularization)

손실 함수 = MSE + λ × (가중치 제곱합)
→ 단순히 MSE만 최소화하지 않고, 가중치가 너무 커지지 않게 벌점도 줌

과대적합 방지 효과

👉 쉽게 말하면: 
선형회귀 직선을 찾는데, 계수(기울기)가 너무 커지면 패널티 줘서
"조금 얌전히, 덜 요동치게" 만드는 방법.
----------------------------------


❤ 라쏘 회귀 (Lasso Regression)

선형 회귀 + L1 규제

손실 함수 = MSE + λ × (가중치 절댓값 합)
→ 일부 계수는 정확히 0이 되므로 변수 선택 효과


👉 쉽게 말하면: 
릿지가 "모든 계수를 조금씩 줄인다"면,
라쏘는 "쓸데없는 계수는 아예 0으로 잘라버린다".
그래서 필요 없는 변수를 자동으로 빼주는 효과가 있음.
-----------------------------------

❤ 로지스틱 회귀 (Logistic Regression)

분류(Classification)에 사용 (0/1, 다중 클래스 확장 가능)

선형 조합 z = w·x + b → 시그모이드 함수에 넣어 확률로 해석
P = 1 / (1 + e^(-z))

손실 함수 = Log Loss (교차 엔트로피 손실)
교차 엔트로피란 ?? >> 실제 정답 분포 𝑦 와 모델이 낸 예측 확률 𝑝 사이의 차이를 재는 것
L = -( y·log(P) + (1-y)·log(1-P) )

특징
- 출력값이 항상 0~1 확률
- 임계값(보통 0.5) 기준으로 클래스 판별
- 계수 양/음 → 특정 특성이 양성(1) 확률을 높이는지 낮추는지 해석 가능
- 다중 클래스 확장: 소프트맥스 회귀(Softmax Regression)

👉 쉽게 말하면: 
"선형회귀 직선"을 그냥 쓰면 0~1 확률로 안 맞아.  
그래서 직선을 'S자 곡선(시그모이드)'로 구부려서 확률처럼 쓰는 게 로지스틱 회귀.
0.5보다 크면 1, 작으면 0이라고 분류.

------------------------------------

❤❤❤ 요약 한 줄 ❤❤❤

KNN 분류: 다수결
KNN 회귀: 평균
MSE (Mean Squared Error) : 평균제곱오차
선형회귀: MSE 최소화
릿지회귀: MSE + L2 규제
라쏘회귀: MSE + L1 규제 (특성 선택)
로지스틱 회귀: 시그모이드 확률 출력 + 로그 손실 최소화


❤ 손실 함수 (Loss Function, 비용 함수 Cost Function)

모델이 얼마나 틀렸는지 측정하는 함수
→ 예측값과 실제값의 차이를 수치로 표현

예시
- 회귀: MSE (Mean Squared Error)
- 분류: Log Loss (교차 엔트로피 손실)

손실 함수 최소화 = 모델 성능 최적화

---------------------------------------


❤ 최소제곱오차 (SSE, Sum of Squared Errors)  
ㄴ 예측값과 실제값 차이를 제곱해서 모두 더한 값
ㄴ SSE: “제곱합” → 단순히 다 더함

오차 제곱의 합
SSE = Σ (y - ŷ)^2

단점: 데이터 개수에 따라 크기가 커짐
→ 평균을 낸 것이 MSE


--------------------------------------

❤ 평균제곱오차 (MSE, Mean Squared Error)

SSE를 샘플 개수 N으로 나눈 값
MSE = (1/N) Σ (y - ŷ)^2

- 선형 회귀의 기본 손실 함수
- 오차가 클수록 제곱해서 크게 벌줌
ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
| SSE: 오차 제곱의 총합                                                      |
| MSE: 오차 제곱의 평균                                                      |
| 둘 다 손실 함수로 쓰이는데, MSE가 더 보편적임 (데이터 크기에 덜 민감하니까).-  |
ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ

❤ 경사 하강법 (Gradient Descent)

손실 함수를 최소화하기 위해 사용하는 반복 최적화 알고리즘

원리
- 기울기(gradient, 미분값) 구하기 
ㄴ어떤 함수가 있을 때, 그 함수가 가장 빨리 증가하는 방향을 나타내는 벡터.
ㄴ쉽게 말하면 → “함수의 경사도”
- 반대 방향으로 조금씩 이동
- 학습률(learning rate) 크기에 따라 이동 거리 결정
- 반복하면서 최솟값 근처 도달

👉 쉽게 말하면: 
산 꼭대기에서 눈덩이를 굴려서 골짜기(최솟값)로 내려보낸다고 생각해.
기울기(경사도)를 따라 내려가는데,
너무 빨리 굴리면(학습률↑) 튕겨 나가고,
너무 천천히 굴리면(학습률↓) 시간이 오래 걸려.


------------------------------------------


❤ 규제항 (Regularization Term)

모델이 과적합하지 않도록 가중치에 벌점 부여

종류
- L2 규제 (Ridge): Σ w^2
   → 모든 가중치를 조금씩 0에 가깝게 줄임
   → 다중공선성 완화, 안정적인 해 제공

- L1 규제 (Lasso): Σ |w|
   → 일부 가중치를 정확히 0으로 만듦
   → 변수 선택(feature selection) 효과

효과
- 가중치가 너무 커지는 것을 방지
- 라쏘는 일부 가중치를 0으로 만들어 변수 선택 효과
- λ(람다, 규제 강도) 값으로 규제의 세기를 조절
   (λ ↑ → 규제 세짐, λ ↓ → 규제 약함)

👉 쉽게 말하면: 
공부를 너무 완벽하게 외우면 시험에선 틀린다 = 과적합.  
그래서 "너무 복잡하지 마!" 하고 벌점 주는 장치.
- L2 (릿지): 살살 줄인다.
- L1 (라쏘): 아예 몇 개는 0으로 없애버린다.
----------------------------------------

❤ 시그모이드 함수 (Sigmoid Function)

출력: 0 ~ 1 사이 확률
σ(x) = 1 / (1 + e^(-x))

특징
- 입력값이 크면 1에 가까움
- 입력값이 작으면 0에 가까움
- 분류 문제에서 확률로 해석 가능

👉 쉽게 말하면: 
아무리 큰 수가 들어와도 0~1 사이로 눌러주는 "스퀴즈" 함수.
그래서 "확률처럼" 쓸 수 있음.
-------------------------------------------

❤ 특성 공학 (Feature Engineering)

원본 데이터를 모델이 학습하기 좋게 가공하는 과정
ㄴ훈련 때 만든 확장 규칙대로, 테스트 데이터도 제곱·곱 특성으로 똑같이 변환한다

방법
- 특성 선택: 불필요한 변수 제거
- 특성 변환: 로그 변환, 제곱, 표준화 등
- 특성 생성: 기존 변수 조합해 새로운 변수 만들기
- 범주형 처리: 원-핫 인코딩, 레이블 인코딩

효과
- 모델 성능 크게 향상
- "좋은 특성이 좋은 모델보다 더 중요하다"는 말이 있을 정도




👉 쉽게 말하면: 
"재료 손질"이랑 똑같아.  
요리를 잘하려면 재료를 먼저 다듬어야 하듯,  
데이터도 제곱, 로그, 인코딩 같은 가공을 해줘야 모델 성능이 확 올라가.

----------------------------------------------
